{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d041452-1f53-4764-8a7a-8af7ef87028a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching https://wehi-researchcomputing.github.io/[url](https:/www.eventbrite.com.au/o/the-rse-association-of-australia-and-new-zealand-65201929823): 404 Client Error: Not Found for url: https://wehi-researchcomputing.github.io/%5Burl%5D(https:/www.eventbrite.com.au/o/the-rse-association-of-australia-and-new-zealand-65201929823)\n",
      "Error fetching https://wehi-researchcomputing.github.io/11-Summer-2024-2025: 404 Client Error: Not Found for url: https://wehi-researchcomputing.github.io/11-Summer-2024-2025\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Function to recursively scrape links\n",
    "def scrape_links(url, root_url, visited=None):\n",
    "    \"\"\"\n",
    "    Crawl through all the sublinks of a root URL, keeping track of what has been visited\n",
    "    Returns a list of strings (URLs)\n",
    "    \"\"\"\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "\n",
    "    # Skip if the URL has already been visited\n",
    "    if url in visited:\n",
    "        return []\n",
    "\n",
    "    # Mark the URL as visited\n",
    "    visited.add(url)\n",
    "\n",
    "    try:\n",
    "        # Fetch the page content\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an error for bad status codes\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract all links from the page\n",
    "    links = []\n",
    "    for a_tag in soup.find_all('a', href=True):\n",
    "        href = a_tag['href']\n",
    "        full_url = urljoin(root_url, href)\n",
    "\n",
    "        # Filter out fragment links, we'll deal with this later\n",
    "        if full_url.startswith(\"https://wehi-researchcomputing.github.io/faq#\") or full_url.startswith(\"https://wehi-researchcomputing.github.io/email\"):\n",
    "            continue  # Skip adding this link\n",
    "\n",
    "        # Only process links that start with the root URL\n",
    "        if full_url.startswith(root_url):\n",
    "            links.append(full_url)\n",
    "            \n",
    "            # Recursively scrape links from the sublink\n",
    "            links.extend(scrape_links(full_url, root_url, visited))\n",
    "\n",
    "    return links\n",
    "\n",
    "# Root URL\n",
    "root_url = \"https://wehi-researchcomputing.github.io/\"\n",
    "\n",
    "# Scrape and store all the links\n",
    "all_links = scrape_links(root_url, root_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c15d59c3-5205-44de-9c9c-c2425ea6914b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://wehi-researchcomputing.github.io/faq', 'https://wehi-researchcomputing.github.io/intakes/', 'https://wehi-researchcomputing.github.io/student-schex', 'https://wehi-researchcomputing.github.io/explanation_about_ohs', 'https://wehi-researchcomputing.github.io/student-genomics-invoicing', 'https://wehi-researchcomputing.github.io/expectations_open_source_contributors', 'https://wehi-researchcomputing.github.io/software_maturity_model', 'https://wehi-researchcomputing.github.io/RDM-0220-RCP-Student-Internship-Handbook.pdf', 'https://wehi-researchcomputing.github.io/student-immunology-web-application', 'https://wehi-researchcomputing.github.io/[url](https:/www.eventbrite.com.au/o/the-rse-association-of-australia-and-new-zealand-65201929823)', 'https://wehi-researchcomputing.github.io/student-aive', 'https://wehi-researchcomputing.github.io/social_media_policy', 'https://wehi-researchcomputing.github.io/student-haemosphere', 'https://wehi-researchcomputing.github.io/student-clinical-dashboards', 'https://wehi-researchcomputing.github.io/intake_dates', 'https://wehi-researchcomputing.github.io/', 'https://wehi-researchcomputing.github.io/student-organiser', 'https://wehi-researchcomputing.github.io/student-duplex-sequencing', 'https://wehi-researchcomputing.github.io/student-quantum', 'https://wehi-researchcomputing.github.io/project-wikis', 'https://wehi-researchcomputing.github.io/student-capacity-planning.html', 'https://wehi-researchcomputing.github.io/student-flux', 'https://wehi-researchcomputing.github.io/11-Summer-2024-2025', 'https://wehi-researchcomputing.github.io/student-immunology-modelling', 'https://wehi-researchcomputing.github.io/student-mixOmics.html', 'https://wehi-researchcomputing.github.io/student-bionix', 'https://wehi-researchcomputing.github.io/student-genomics-metadata.html', 'https://wehi-researchcomputing.github.io/students', 'https://wehi-researchcomputing.github.io/top-5-mistakes', 'https://wehi-researchcomputing.github.io/student-data-commons', 'https://wehi-researchcomputing.github.io/assets/rcp_logo.png', 'https://wehi-researchcomputing.github.io/student-cryoem', 'https://wehi-researchcomputing.github.io/how-to-apply', 'https://wehi-researchcomputing.github.io/complex-projects', 'https://wehi-researchcomputing.github.io/student-genomics-qc', 'https://wehi-researchcomputing.github.io/student-imaging', 'https://wehi-researchcomputing.github.io/student-loxcoder', 'https://wehi-researchcomputing.github.io/code-of-conduct']\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates \n",
    "urls = list(set(all_links))\n",
    "\n",
    "# Checking the URLs that were scraped\n",
    "print(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd780412-cefd-4a8f-a9f1-ec5ad900ad62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Add additional files that should be entered into the vector database\n",
    "\"\"\"\n",
    "\n",
    "# List of PDF files to load\n",
    "pdf_files = [\n",
    "    \"RCP0032 Intake 10 Student Internship Summary Reports.pdf\",\n",
    "    \"Research Computing Platform Student Internship Handbook.pdf\",\n",
    "    \"Student Projects Outline - Summer 2425.pdf\"\n",
    "]\n",
    "\n",
    "# Additional URLs + bad practice strategy to add \"weighting\" to the FAQ page\n",
    "additional_urls = [\n",
    "    \"https://wehi-researchcomputing.github.io/faq\"\n",
    "    \"https://wehi-researchcomputing.github.io/intake_dates\"\n",
    "]\n",
    "\n",
    "urls += additional_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6feb7a4-68c8-4adc-a481-5b6b60d2a671",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 31 0 (offset 0)\n",
      "Ignoring wrong pointing object 36 0 (offset 0)\n",
      "Ignoring wrong pointing object 41 0 (offset 0)\n",
      "Ignoring wrong pointing object 58 0 (offset 0)\n",
      "Ignoring wrong pointing object 88 0 (offset 0)\n",
      "Ignoring wrong pointing object 101 0 (offset 0)\n",
      "Ignoring wrong pointing object 110 0 (offset 0)\n",
      "Ignoring wrong pointing object 119 0 (offset 0)\n",
      "Ignoring wrong pointing object 136 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching https://wehi-researchcomputing.github.io/[url](https:/www.eventbrite.com.au/o/the-rse-association-of-australia-and-new-zealand-65201929823): 404 Client Error: Not Found for url: https://wehi-researchcomputing.github.io/%5Burl%5D(https:/www.eventbrite.com.au/o/the-rse-association-of-australia-and-new-zealand-65201929823)\n",
      "Error fetching https://wehi-researchcomputing.github.io/11-Summer-2024-2025: 404 Client Error: Not Found for url: https://wehi-researchcomputing.github.io/11-Summer-2024-2025\n",
      "Error fetching https://wehi-researchcomputing.github.io/faqhttps://wehi-researchcomputing.github.io/intake_dates: 404 Client Error: Not Found for url: https://wehi-researchcomputing.github.io/faqhttps://wehi-researchcomputing.github.io/intake_dates\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Function to scrape page content and extract headers with section IDs\n",
    "def scrape_page(url):\n",
    "    \"\"\"\n",
    "    Scrape individual pages to retrieve all of their section ID within the URL.\n",
    "\n",
    "    ie.         https://wehi-researchcomputing.github.io/faq#how-should-i-ask-for-help-to-solve-a-problem\n",
    "    instead of: https://wehi-researchcomputing.github.io/faq\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # Extract main text content\n",
    "    page_text = soup.get_text(separator=\"\\n\")\n",
    "    \n",
    "    # Extract headers with IDs\n",
    "    section_ids = {}\n",
    "    for header in soup.find_all([\"h2\"]):\n",
    "        section_text = header.get_text(strip=True)\n",
    "        section_id = header.get(\"id\")  # Extract ID if available\n",
    "        if section_id and section_id not in section_ids.values():\n",
    "            section_ids[section_text] = section_id  # Store mapping of header text -> ID\n",
    "\n",
    "    return page_text, section_ids\n",
    "\n",
    "\"\"\"\n",
    "Load PDF content and webpage content + metadata of all of the sources into all_docs\n",
    "\"\"\"\n",
    "\n",
    "# Initialize an empty list to store all documents\n",
    "all_docs = []\n",
    "\n",
    "# Load PDFs with metadata\n",
    "for pdf_file in pdf_files:\n",
    "    pdf_loader = PyPDFLoader(pdf_file)\n",
    "    pdf_docs = pdf_loader.load()\n",
    "    for doc in pdf_docs:\n",
    "        doc.metadata[\"source\"] = pdf_file  # Add source metadata\n",
    "    all_docs.extend(pdf_docs)\n",
    "\n",
    "for url in urls:\n",
    "    # Scrape page to extract section headers\n",
    "    _, section_ids = scrape_page(url)  # Get section mapping\n",
    "\n",
    "    # Load webpage content using WebBaseLoader\n",
    "    url_loader = WebBaseLoader(url)\n",
    "    web_docs = url_loader.load()\n",
    "    \n",
    "    for doc in web_docs:\n",
    "        doc.metadata[\"source\"] = url  # Store the source URL\n",
    "        doc.metadata[\"section_ids\"] = section_ids  # Store extracted section headers\n",
    "\n",
    "    all_docs.extend(web_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2343858-9524-40e3-8555-60a941f55a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split documents into chunks, so that when retrieving context, the entire document is not loaded (wasted context window),\n",
    "but instead only the relevant chunks are added. Experiment with chunk_size and chunk_overlap parameters\n",
    "\"\"\"\n",
    "\n",
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "734b9f16-b35b-47dc-a324-cda28853b581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the most relevant section ID for a chunk\n",
    "def find_section_id(chunk_text, section_ids):\n",
    "    \"\"\"\n",
    "    Find the most relevant section ID for a specific chunk.\n",
    "\n",
    "    Returns the section_id (ie. how-should-i-ask-for-help-to-solve-a-problem)\n",
    "    \"\"\"\n",
    "    if not section_ids:\n",
    "        return None\n",
    "        \n",
    "    for header, section_id in section_ids.items():\n",
    "        if header in chunk_text:  # If the header appears in the chunk, use its ID\n",
    "            return section_id\n",
    "    return None  # No matching section ID found\n",
    "\n",
    "# Assign section IDs to chunks\n",
    "for chunk in chunks:\n",
    "    \"\"\"\n",
    "    Find the relevant section ID's and add to metadata\n",
    "    Testing adding section ID to page content as well for better retrieval if query resembles section ID\n",
    "    \"\"\"\n",
    "    section_id = find_section_id(chunk.page_content, chunk.metadata.get(\"section_ids\", {}))\n",
    "    if section_id:\n",
    "        chunk.metadata[\"section_id\"] = section_id  # Store section ID\n",
    "        chunk.page_content += f\"\\nSection ID: {section_id.replace('-', ' ')}\"\n",
    "\n",
    "# Ensure sources include section IDs in retrieval\n",
    "for chunk in chunks:\n",
    "    \"\"\"\n",
    "    Add section Id's to source, clean up chunk metadata\n",
    "    \"\"\"\n",
    "    if \"section_id\" in chunk.metadata:\n",
    "        chunk.metadata[\"source\"] = f\"{chunk.metadata['source']}#{chunk.metadata['section_id']}\"\n",
    "    chunk.metadata.pop(\"section_ids\", None)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56350671-eb33-45f9-b7dd-881a5764ffef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/11/k69l5llx455_fx0m3hqwgc280000gn/T/ipykernel_49059/3212785722.py:16: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vector_store = Chroma(persist_directory=\"./chroma_db\", embedding_function=embedding_model)\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "\"\"\"\n",
    "Initialise embedding model + vector store using ChromaDB\n",
    "Create new vector store/load existing vector store\n",
    "\"\"\"\n",
    "\n",
    "# Initialize embedding model \"all-MiniLM-L6-v2\"\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create vector store\n",
    "vector_store = Chroma.from_documents(chunks, embedding_model, persist_directory=\"./chroma_db\")\n",
    "\n",
    "# Load the existing ChromaDB database\n",
    "vector_store = Chroma(persist_directory=\"./chroma_db\", embedding_function=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78ba8e51-c2be-4600-8799-e2a70db6e752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom huggingface_hub import hf_hub_download\\nfrom langchain_community.llms import GPT4All\\n\\nmodel_path = \"models\"\\nmodel_name = \"bling-phi-3.gguf\"\\nhf_hub_download(repo_id=\"llmware/bling-phi-3-gguf\", filename=model_name, local_dir=model_path)\\n\\nllm = GPT4All(model=\"./models/bling-phi-3.gguf\")\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "\"\"\"\n",
    "Iniitalise LLM, bling-phi-3-gguf\n",
    "\n",
    "For local deployment, Ollama is recommended.\n",
    "For external deployment, it is possible to download the chosen LLM model via hf_hub_download\n",
    "and load the model using GPT4ALL\n",
    "\"\"\"\n",
    "\n",
    "# LLM Model\n",
    "MODEL = \"hf.co/llmware/bling-phi-3-gguf\"\n",
    "llm = OllamaLLM(model=MODEL)\n",
    "\n",
    "\"\"\"\n",
    "from huggingface_hub import hf_hub_download\n",
    "from langchain_community.llms import GPT4All\n",
    "\n",
    "model_path = \"models\"\n",
    "model_name = \"bling-phi-3.gguf\"\n",
    "hf_hub_download(repo_id=\"llmware/bling-phi-3-gguf\", filename=model_name, local_dir=model_path)\n",
    "\n",
    "llm = GPT4All(model=\"./models/bling-phi-3.gguf\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7197894-f252-4da3-a580-ede88ea92d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\"\"\"\n",
    "Create Langchain RAG Pipeline, combining prompt, vector store retrieval and LLM\n",
    "\"\"\"\n",
    "\n",
    "template = \"\"\"{context}\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Please provide a clear and concise answer based only on the information provided above. \n",
    "        If the information is not sufficient to answer the question, please say so.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "# Define a new chain to return both the answer and the sources\n",
    "qa_chain_with_sources = (\n",
    "    RunnableParallel(\n",
    "        {\n",
    "            \"context\": vector_store.as_retriever(),\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "    )\n",
    "    | {\n",
    "        \"answer\": prompt | llm | StrOutputParser(),\n",
    "        \"sources\": lambda x: [doc.metadata.get(\"source\", \"Unknown\") for doc in x[\"context\"]],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a92e2f33-95d5-4c07-823a-d9eb0d72fd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to call a RAG LLM query\n",
    "def rag_query(query, history):\n",
    "    \"\"\"\n",
    "    2 Parameters: Query --> string user query, history --> chat history\n",
    "    \n",
    "    Invoke the Langchain RAG sequence, and retrieve sources. History is currently unused due to limited context window available,\n",
    "    may implement in the future\n",
    "    \n",
    "    Returns formatted string output for the chatbot\n",
    "    \"\"\"\n",
    "    # Invoke the chain\n",
    "    response = qa_chain_with_sources.invoke(query)\n",
    "    \n",
    "    answer = response[\"answer\"]\n",
    "    unique_sources = list(set(response[\"sources\"]))\n",
    "\n",
    "    # Print answers + sources\n",
    "    output = f\"Answer: {answer}\\n\\nSources:\\n\" + \"\\n\".join(unique_sources)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94551f80-5818-44a0-8aad-8e32496c7088",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\"\"\"\n",
    "Gradio chat interface calling the rag_query as the function\n",
    "Added example queries to give users an idea of what type of queries can be asked/answered\n",
    "\"\"\"\n",
    "\n",
    "# Create Gradio ChatInterface\n",
    "iface = gr.ChatInterface(\n",
    "    fn=rag_query,  # Function to call for generating responses\n",
    "    title=\"WEHI Student Intern Chatbot Demo\",\n",
    "    type='messages',\n",
    "    description=\"Ask questions related to your WEHI internship and get answers with sources.\",\n",
    "    examples=[\n",
    "        \"What flexibility is there for the internship?\",\n",
    "        \"What are the tasks for the REDMANE Data Ingestion team?\",\n",
    "        \"What are the key things to do before the weekly meetings?\", \n",
    "        \"How do I tackle complex and ambiguous projects?\",\n",
    "        \"What happens over Easter break at WEHI?\",\n",
    "        \"When is the final presentation due?\",\n",
    "        \"What is Nectar?\",\n",
    "        \"Is the internship remote or in person?\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Launch Gradio interface, share=True opens a public URL for 72-hour demo\n",
    "iface.launch()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d90881-2e6d-479d-a81c-50c9c57a4954",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
